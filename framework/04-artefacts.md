# Les Artefacts Vivants

## Principe Cardinal

Les artefacts AIAD ne sont pas de la bureaucratie. Ce sont des **outils de pensÃ©e et de communication** qui crÃ©ent de l'alignement et rÃ©duisent l'ambiguÃ¯tÃ©.

**Un bon artefact AIAD est :**
- âœ… **Actionnable** : On peut agir Ã  partir de lui
- âœ… **Vivant** : Il Ã©volue avec la comprÃ©hension
- âœ… **Minimal** : Juste assez, pas plus
- âœ… **Collaboratif** : Co-crÃ©Ã©, pas dictÃ©

## Les 4 Artefacts Essentiels

### 1. Product Requirement Document (PRD)

**Objectif :** Clarifier POURQUOI et QUOI construire avant de se poser la question du COMMENT.

**Sections clÃ©s :**
- **Contexte et ProblÃ¨me** : Quel problÃ¨me ? Pour qui ? Pourquoi maintenant ?
- **Success Criteria (Outcome Criteria)** : MÃ©triques mesurables et observables
- **Personas et Use Cases** : Profils utilisateurs et scÃ©narios d'usage
- **Hors PÃ©rimÃ¨tre** : Ce que nous NE faisons PAS (volontairement)
- **Trade-offs et DÃ©cisions** : DÃ©cisions majeures prises et alternatives Ã©cartÃ©es
- **DÃ©pendances et Risques** : PrÃ©requis et risques identifiÃ©s

**Bonnes pratiques :**
1. Commencer par le problÃ¨me, pas par la solution
2. Outcome-driven : DÃ©finir success criteria mesurables avant de construire
3. Collaboratif : RÃ©diger avec l'Ã©quipe, pas pour l'Ã©quipe
4. ItÃ©ratif : Le PRD Ã©volue avec la comprÃ©hension
5. Visuel : IntÃ©grer wireframes, flows quand pertinent

**Anti-patterns :**
- ğŸš« PRD fleuve de 50 pages que personne ne lira
- ğŸš« PRD dictatorial avec solution imposÃ©e
- ğŸš« PRD vague : "AmÃ©liorer l'expÃ©rience utilisateur"
- ğŸš« PRD statique jamais mis Ã  jour

> ğŸ“– *Voir Annexe A.1 pour le template complet et exemples*

---

### 2. Document ARCHITECTURE

**Objectif :** DÃ©finir les standards techniques que les agents IA et les PE doivent respecter.

**Sections clÃ©s :**
- **Principes Architecturaux** : 5 principes non-nÃ©gociables
- **Vue d'Ensemble** : Architecture high-level avec justification
- **Stack Technique** : Technologies, versions, justifications
- **Structure du Projet** : Organisation dossiers et modules
- **Conventions de Code** : Nommage, formatage, imports
- **Patterns et Bonnes Pratiques** : Design patterns avec exemples
- **SÃ©curitÃ©** : Principes et pratiques obligatoires
- **Performance** : Budgets de performance et stratÃ©gies
- **QualitÃ© et Dette Technique** : Definition of Output Done, gestion dette
- **ADR (Architecture Decision Records)** : DÃ©cisions majeures documentÃ©es

**Bonnes pratiques :**
1. Ã‰volutif, pas figÃ© : L'architecture Ã©volue avec le produit
2. JustifiÃ© : Chaque choix a une rationale explicite
3. Pragmatique : Pas d'over-engineering, YAGNI
4. Communicable : Diagrammes visuels, pas que du texte
5. Actionnable : Les PE peuvent s'y rÃ©fÃ©rer quotidiennement

**Anti-patterns :**
- ğŸš« Ivory tower architecture : DÃ©cider sans connaÃ®tre la rÃ©alitÃ©
- ğŸš« CV-driven development : Choisir des technos pour le CV
- ğŸš« Documentation obsolÃ¨te qui ne reflÃ¨te pas la rÃ©alitÃ©
- ğŸš« ComplexitÃ© prÃ©maturÃ©e pour des problÃ¨mes hypothÃ©tiques

> ğŸ“– *Voir Annexe A.2 pour le template complet et exemples d'ADR*

---

### 3. AGENT-GUIDE (ex. CLAUDE.md)

**Objectif :** Fournir le contexte optimal aux agents IA pour qu'ils gÃ©nÃ¨rent du code de qualitÃ© alignÃ© avec les standards de l'Ã©quipe.

**Principe cardinal :** Un agent sans contexte gÃ©nÃ¨re du code gÃ©nÃ©rique. Un agent avec un contexte riche gÃ©nÃ¨re du code professionnel.

**Sections clÃ©s :**
- **IdentitÃ© du Projet** : Nom, description, domaine mÃ©tier, mission
- **Documentation de RÃ©fÃ©rence** : Liens vers PRD, ARCHITECTURE, SPECs
- **Stack Technique** : RÃ©sumÃ© des technologies utilisÃ©es
- **RÃ¨gles Absolues** : TOUJOURS (obligations) et JAMAIS (interdictions)
- **Conventions de Code** : Nommage, organisation imports, structure composants
- **Vocabulaire MÃ©tier** : Termes spÃ©cifiques au domaine avec dÃ©finitions
- **Configuration des Agents SpÃ©cialisÃ©s** : RÃ¨gles spÃ©cifiques par agent
- **Patterns de DÃ©veloppement** : Approches favorisÃ©es avec exemples
- **Anti-Patterns** : Ce qu'il faut Ã©viter avec exemples
- **Notes d'Apprentissage** : Learnings mis Ã  jour continuellement

**Bonnes pratiques :**
1. Concret et actionnable : Exemples de code, pas juste principes abstraits
2. Ã‰volutif : Section "Notes d'Apprentissage" mise Ã  jour continuellement
3. Contextuel : Vocabulaire mÃ©tier spÃ©cifique au domaine
4. Ã‰quilibrÃ© : Ne pas tomber dans l'excÃ¨s (100+ rÃ¨gles = aucune rÃ¨gle)
5. Vivant : Review mensuelle minimum pour synchroniser avec la rÃ©alitÃ©

**Anti-patterns :**
- ğŸš« Guide encyclopÃ©dique de 50 pages que personne ne lit
- ğŸš« Guide obsolÃ¨te Ã©crit au dÃ©but puis jamais mis Ã  jour
- ğŸš« Guide vague : "Ã‰crire du bon code" (pas actionnable)
- ğŸš« Guide dictatorial avec trop de rÃ¨gles rigides

> ğŸ“– *Voir Annexe A.3 pour le template complet avec exemples de rÃ¨gles*

---

### 4. Document SPECS (SpÃ©cifications Techniques)

**Objectif :** Bridge entre l'intention mÃ©tier (PRD) et l'implÃ©mentation concrÃ¨te par les agents IA.

**Principe cardinal :** Une SPEC de qualitÃ© permet Ã  un agent IA de gÃ©nÃ©rer 80%+ du code correct du premier coup.

**Sections clÃ©s :**
- **Contexte** : RÃ©fÃ©rence User Story, objectif, outcome attendu
- **PÃ©rimÃ¨tre** : In Scope / Out of Scope explicites
- **Fichiers ImpactÃ©s** : Ã€ crÃ©er / Ã€ modifier
- **Interface Technique** : API endpoints, types, schÃ©mas DB
- **Comportement DÃ©taillÃ©** : Flow nominal et cas limites
- **Validation Rules** : RÃ¨gles de validation avec schÃ©mas
- **Business Rules** : RÃ¨gles mÃ©tier Ã  appliquer
- **Tests Attendus** : ScÃ©narios de test Ã  implÃ©menter
- **Exemples d'Usage** : Exemples concrets requÃªte/rÃ©ponse
- **Outcome Criteria** : Comment mesurer le succÃ¨s (si applicable)
- **DÃ©pendances** : PrÃ©-requis et impacts downstream
- **Definition of Output Done Checklist** : CritÃ¨res de "Done"

**CritÃ¨res d'une SPEC de qualitÃ© :**

| CritÃ¨re | Bon Exemple | Mauvais Exemple |
|---------|-------------|-----------------|
| **AtomicitÃ©** | "ImplÃ©menter crÃ©ation tÃ¢che via API" | "Faire module gestion de tÃ¢ches" |
| **PrÃ©cision** | "Retourner 400 si title vide ou >200 chars" | "GÃ©rer les erreurs" |
| **TestabilitÃ©** | "Accepter 'test@example.com', rejeter 'invalid'" | "Tester la validation" |
| **ComplÃ©tude** | Inclut types, validation, edge cases, tests | "CrÃ©er endpoint POST" |

**Anti-patterns :**
- ğŸš« SPEC vague : "AmÃ©liorer la performance" (non actionnable)
- ğŸš« SPEC tentaculaire : 20 fonctionnalitÃ©s dans une SPEC
- ğŸš« SPEC code : Ã‰crire le code directement dans la SPEC
- ğŸš« SPEC obsolÃ¨te jamais mise Ã  jour avec les learnings

> ğŸ“– *Voir Annexe A.4 pour le template complet et exemples de SPECs*

---

## Definitions of Done

### Definition of Output Done (DoOD)

**Objectif :** Standard de qualitÃ© uniforme et non-nÃ©gociable pour qu'un incrÃ©ment soit considÃ©rÃ© comme "Done" et livrable.

**Principe :** Une fonctionnalitÃ© n'est "Done" que si TOUS les critÃ¨res sont satisfaits. Pas d'exception. Pas de "Done Ã  90%".

**CatÃ©gories de critÃ¨res :**

**CritÃ¨res Techniques :**
- Code Quality (conventions, linting, types, complexitÃ©, commentaires)
- Testing (couverture, tous tests passent, edge cases, pas de flaky tests)
- Security (scan passÃ©, pas de secrets, validation inputs, gestion erreurs)
- Performance (budgets respectÃ©s, queries optimisÃ©es, monitoring)

**CritÃ¨res Fonctionnels :**
- ConformitÃ© (spec respectÃ©e, acceptance criteria validÃ©s)
- Documentation (API, README, CHANGELOG)

**CritÃ¨res de DÃ©ploiement :**
- CI/CD (build rÃ©ussit, dÃ©ployÃ© staging, smoke tests, rollback plan)
- Review (code review, QA validation, PM validation si pertinent)

**CritÃ¨res de QualitÃ© Long-Terme :**
- MaintenabilitÃ© (pas de dette ou dette documentÃ©e, architecture cohÃ©rente)

**Ce qui N'est PAS "Done" :**
- âŒ "Le code compile"
- âŒ "Ã‡a marche sur ma machine"
- âŒ "Les tests passent localement"
- âŒ "Done Ã  90%"
- âŒ "On testera/documentera plus tard"

> ğŸ“– *Voir Annexe A.5 pour la checklist complÃ¨te et exemples*

---

### Definition of Outcome Done (DoOuD)

**Objectif :** DÃ©finir comment mesurer si la valeur attendue a Ã©tÃ© rÃ©alisÃ©e pour les stakeholders.

**Principe :** L'output (code livrÃ©) n'est que le moyen. L'outcome (valeur rÃ©alisÃ©e) est le but.

**CatÃ©gories de mÃ©triques :**

**User Outcomes :**
- Satisfaction utilisateur (NPS, CSAT)
- Adoption fonctionnalitÃ©
- Time to Value
- Retention

**Business Outcomes :**
- Impact business (MRR, conversions, etc.)
- EfficacitÃ© opÃ©rationnelle
- RÃ©duction coÃ»ts

**Learning Outcomes :**
- HypothÃ¨ses validÃ©es/invalidÃ©es
- Insights utilisateur dÃ©couverts
- Learnings techniques

**Process de mesure :**
1. DÃ©finir outcomes AVANT de construire
2. Mesurer Ã  des jalons dÃ©finis (1 sem, 1 mois, 3 mois)
3. Comparer attendu vs. rÃ©alisÃ©
4. DÃ©cider : Continuer / ItÃ©rer / Sunset
5. Documenter learnings

> ğŸ“– *Voir Annexe A.6 pour le template complet et exemples de mÃ©triques*
